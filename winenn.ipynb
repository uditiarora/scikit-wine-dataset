{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uditi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "Y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "(178, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>0.938202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>0.775035</td>\n",
       "      <td>0.775035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
       "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
       "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
       "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
       "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
       "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
       "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
       "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
       "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
       "\n",
       "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "count     178.000000  178.000000            178.000000       178.000000   \n",
       "mean        2.295112    2.029270              0.361854         1.590899   \n",
       "std         0.625851    0.998859              0.124453         0.572359   \n",
       "min         0.980000    0.340000              0.130000         0.410000   \n",
       "25%         1.742500    1.205000              0.270000         1.250000   \n",
       "50%         2.355000    2.135000              0.340000         1.555000   \n",
       "75%         2.800000    2.875000              0.437500         1.950000   \n",
       "max         3.880000    5.080000              0.660000         3.580000   \n",
       "\n",
       "       color_intensity         hue  od280/od315_of_diluted_wines      proline  \\\n",
       "count       178.000000  178.000000                    178.000000   178.000000   \n",
       "mean          5.058090    0.957449                      2.611685   746.893258   \n",
       "std           2.318286    0.228572                      0.709990   314.907474   \n",
       "min           1.280000    0.480000                      1.270000   278.000000   \n",
       "25%           3.220000    0.782500                      1.937500   500.500000   \n",
       "50%           4.690000    0.965000                      2.780000   673.500000   \n",
       "75%           6.200000    1.120000                      3.170000   985.000000   \n",
       "max          13.000000    1.710000                      4.000000  1680.000000   \n",
       "\n",
       "           target       class  \n",
       "count  178.000000  178.000000  \n",
       "mean     0.938202    0.938202  \n",
       "std      0.775035    0.775035  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    0.000000  \n",
       "50%      1.000000    1.000000  \n",
       "75%      2.000000    2.000000  \n",
       "max      2.000000    2.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(X)\n",
    "print(wine.feature_names)\n",
    "data.columns = wine.feature_names\n",
    "data['target'] = Y\n",
    "data['class'] = Y\n",
    "print(data.shape)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ohe = np.reshape(Y,(len(Y),1))\n",
    "enc = OneHotEncoder()\n",
    "Y_ohe = enc.fit_transform(Y_ohe).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_ohe) # one-hot-encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = model_selection.train_test_split(X,Y_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights & biases\n",
    "n_input = 13\n",
    "n_hidden_1 = 16\n",
    "n_hidden_2 = 8\n",
    "n_classes = 3\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(13, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_2:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_3:0' shape=(16,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_4:0' shape=(8,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_5:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, weights, biases):\n",
    "    in_layer1 = tf.add(tf.matmul(x, weights['h1']), biases['h1'])\n",
    "    out_layer1 = tf.nn.relu(in_layer1)\n",
    "    \n",
    "    in_layer2 = tf.add(tf.matmul(out_layer1, weights['h2']), biases['h2'])\n",
    "    out_layer2 = tf.nn.relu(in_layer2)\n",
    "    \n",
    "    output = tf.add(tf.matmul(out_layer2, weights['out']), biases['out'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None, n_classes])\n",
    "pred = forward_propagation(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-680970d64d68>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels = y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "optimize = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2158.116\n",
      "1953.6259\n",
      "1751.2267\n",
      "1550.8126\n",
      "1352.5277\n",
      "1156.3444\n",
      "963.642\n",
      "775.0457\n",
      "590.6835\n",
      "413.10635\n",
      "245.91386\n",
      "262.015\n",
      "367.04465\n",
      "428.0822\n",
      "456.11435\n",
      "459.15433\n",
      "442.19348\n",
      "408.45453\n",
      "359.83948\n",
      "298.31885\n",
      "225.11328\n",
      "140.8727\n",
      "69.410324\n",
      "67.260605\n",
      "96.210556\n",
      "118.14897\n",
      "126.34625\n",
      "120.25902\n",
      "102.027016\n",
      "73.999664\n",
      "40.00199\n",
      "26.712038\n",
      "66.137695\n",
      "88.74092\n",
      "84.96435\n",
      "58.720203\n",
      "29.324175\n",
      "27.370087\n",
      "39.131924\n",
      "47.070423\n",
      "46.381237\n",
      "37.78095\n",
      "24.819525\n",
      "14.564144\n",
      "12.360349\n",
      "17.004478\n",
      "24.547983\n",
      "25.779634\n",
      "20.553288\n",
      "16.493553\n",
      "15.843816\n",
      "17.197727\n",
      "18.821873\n",
      "19.887505\n",
      "19.702679\n",
      "18.162834\n",
      "15.617275\n",
      "12.872952\n",
      "11.507161\n",
      "11.774977\n",
      "13.244779\n",
      "14.407668\n",
      "14.564068\n",
      "13.695275\n",
      "12.4379225\n",
      "11.768358\n",
      "11.679211\n",
      "11.987608\n",
      "12.390124\n",
      "12.330786\n",
      "11.731001\n",
      "11.030294\n",
      "10.599646\n",
      "10.4661\n",
      "10.62584\n",
      "10.89917\n",
      "11.034864\n",
      "10.825829\n",
      "10.470606\n",
      "10.205997\n",
      "10.114531\n",
      "10.093192\n",
      "10.075726\n",
      "10.006351\n",
      "9.857957\n",
      "9.675368\n",
      "9.542057\n",
      "9.493813\n",
      "9.480995\n",
      "9.46391\n",
      "9.408181\n",
      "9.288439\n",
      "9.134505\n",
      "8.99992\n",
      "8.896316\n",
      "8.827617\n",
      "8.768684\n",
      "8.703856\n",
      "8.615711\n",
      "8.501675\n",
      "8.37843\n",
      "8.265545\n",
      "8.179737\n",
      "8.107523\n",
      "8.015469\n",
      "7.9064775\n",
      "7.791249\n",
      "7.6772575\n",
      "7.567037\n",
      "7.4549737\n",
      "7.335842\n",
      "7.2077093\n",
      "7.070724\n",
      "6.919636\n",
      "6.780721\n",
      "6.6493015\n",
      "6.5351143\n",
      "6.4398646\n",
      "6.345096\n",
      "6.2562647\n",
      "6.196865\n",
      "6.1959906\n",
      "6.1803327\n",
      "6.143421\n",
      "6.098434\n",
      "6.0750713\n",
      "6.0406365\n",
      "5.991095\n",
      "5.937877\n",
      "5.898757\n",
      "5.850074\n",
      "5.790072\n",
      "5.7387314\n",
      "5.6930194\n",
      "5.6422124\n",
      "5.583989\n",
      "5.528062\n",
      "5.481062\n",
      "5.429391\n",
      "5.371089\n",
      "5.3189144\n",
      "5.270024\n",
      "5.216777\n",
      "5.162775\n",
      "5.114276\n",
      "5.067371\n",
      "5.0174246\n",
      "4.9692755\n",
      "4.924523\n",
      "4.879305\n",
      "4.83383\n",
      "4.796933\n",
      "4.7665653\n",
      "4.7350035\n",
      "4.7043767\n",
      "4.675909\n",
      "4.64771\n",
      "4.619305\n",
      "4.5921454\n",
      "4.5661983\n",
      "4.540268\n",
      "4.514644\n",
      "4.4902225\n",
      "4.466521\n",
      "4.442805\n",
      "4.419413\n",
      "4.3968086\n",
      "4.374531\n",
      "4.350842\n",
      "4.3201523\n",
      "4.2895923\n",
      "4.259216\n",
      "4.2288213\n",
      "4.199226\n",
      "4.170865\n",
      "4.1432586\n",
      "4.116673\n",
      "4.091877\n",
      "4.0682063\n",
      "4.043954\n",
      "4.018221\n",
      "3.9920232\n",
      "3.9658413\n",
      "3.9394119\n",
      "3.9128911\n",
      "3.8863096\n",
      "3.8602347\n",
      "3.8347025\n",
      "3.8099763\n",
      "3.7859707\n",
      "3.7620559\n",
      "3.7381265\n",
      "3.7145393\n",
      "3.6911514\n",
      "3.6676626\n",
      "3.6441712\n",
      "3.6209323\n",
      "3.5980155\n",
      "3.5755758\n",
      "3.5536618\n",
      "3.5324228\n",
      "3.5130982\n",
      "3.4942229\n",
      "3.4756367\n",
      "3.45719\n",
      "3.4388897\n",
      "3.4207778\n",
      "3.40288\n",
      "3.3852127\n",
      "3.3678048\n",
      "3.3505654\n",
      "3.3335063\n",
      "3.316674\n",
      "3.3000286\n",
      "3.2835\n",
      "3.2666173\n",
      "3.2455778\n",
      "3.223162\n",
      "3.2005625\n",
      "3.1781664\n",
      "3.156034\n",
      "3.1342978\n",
      "3.1135826\n",
      "3.0950003\n",
      "3.078351\n",
      "3.061528\n",
      "3.0433052\n",
      "3.0245018\n",
      "3.0056481\n",
      "2.9859958\n",
      "2.9655373\n",
      "2.9457867\n",
      "2.9278562\n",
      "2.9108024\n",
      "2.8932023\n",
      "2.8752604\n",
      "2.8578289\n",
      "2.8406584\n",
      "2.8228617\n",
      "2.8045132\n",
      "2.7866178\n",
      "2.7696016\n",
      "2.752736\n",
      "2.7356443\n",
      "2.7191312\n",
      "2.7030232\n",
      "2.6864219\n",
      "2.669349\n",
      "2.6526074\n",
      "2.636332\n",
      "2.620054\n",
      "2.6038146\n",
      "2.5879703\n",
      "2.5721269\n",
      "2.5559816\n",
      "2.5407882\n",
      "2.5259206\n",
      "2.5111127\n",
      "2.4963646\n",
      "2.4818718\n",
      "2.4675713\n",
      "2.453168\n",
      "2.4387205\n",
      "2.4243574\n",
      "2.4100294\n",
      "2.3957694\n",
      "2.3816743\n",
      "2.3676865\n",
      "2.3537018\n",
      "2.3397145\n",
      "2.325799\n",
      "2.311909\n",
      "2.2980754\n",
      "2.284373\n",
      "2.270737\n",
      "2.2571418\n",
      "2.2435856\n",
      "2.2300832\n",
      "2.216623\n",
      "2.203238\n",
      "2.1899486\n",
      "2.176719\n",
      "2.1635344\n",
      "2.1504114\n",
      "2.1373425\n",
      "2.1243322\n",
      "2.1113894\n",
      "2.0985298\n",
      "2.085734\n",
      "2.0729952\n",
      "2.0603201\n",
      "2.047703\n",
      "2.0351524\n",
      "2.0227304\n",
      "2.010416\n",
      "1.9981722\n",
      "1.9859955\n",
      "1.9738951\n",
      "1.9618424\n",
      "1.949882\n",
      "1.93798\n",
      "1.9261428\n",
      "1.914377\n",
      "1.902675\n",
      "1.8910366\n",
      "1.8794612\n",
      "1.867958\n",
      "1.8565153\n",
      "1.8451362\n",
      "1.8338239\n",
      "1.8225775\n",
      "1.8113935\n",
      "1.8002733\n",
      "1.7892287\n",
      "1.7782378\n",
      "1.76734\n",
      "1.7565249\n",
      "1.7457789\n",
      "1.7351158\n",
      "1.7245027\n",
      "1.7139611\n",
      "1.7034987\n",
      "1.6931101\n",
      "1.6827955\n",
      "1.6725373\n",
      "1.6623551\n",
      "1.6522408\n",
      "1.6421924\n",
      "1.6322143\n",
      "1.6223018\n",
      "1.6124617\n",
      "1.6026814\n",
      "1.592969\n",
      "1.5833321\n",
      "1.5737511\n",
      "1.5642351\n",
      "1.5547857\n",
      "1.5453968\n",
      "1.5360765\n",
      "1.5268143\n",
      "1.5176255\n",
      "1.5084937\n",
      "1.4994224\n",
      "1.4904171\n",
      "1.481466\n",
      "1.4725885\n",
      "1.4637593\n",
      "1.455017\n",
      "1.4463573\n",
      "1.4377577\n",
      "1.4292339\n",
      "1.420762\n",
      "1.4123614\n",
      "1.4040139\n",
      "1.3957243\n",
      "1.3874978\n",
      "1.3793305\n",
      "1.3712152\n",
      "1.3631595\n",
      "1.3551598\n",
      "1.3472148\n",
      "1.3393245\n",
      "1.3314893\n",
      "1.3237075\n",
      "1.3159798\n",
      "1.3083\n",
      "1.300671\n",
      "1.2931002\n",
      "1.285573\n",
      "1.2780945\n",
      "1.2706666\n",
      "1.2633348\n",
      "1.2559682\n",
      "1.2486862\n",
      "1.2414552\n",
      "1.2342656\n",
      "1.2271053\n",
      "1.2199922\n",
      "1.2129105\n",
      "1.2058719\n",
      "1.198875\n",
      "1.1919181\n",
      "1.1849904\n",
      "1.1780385\n",
      "1.1676551\n",
      "1.1565409\n",
      "1.1469786\n",
      "1.1397235\n",
      "1.1331121\n",
      "1.1259067\n",
      "1.1187961\n",
      "1.1115538\n",
      "1.1035423\n",
      "1.0953959\n",
      "1.0872881\n",
      "1.078673\n",
      "1.0700324\n",
      "1.0616081\n",
      "1.0531343\n",
      "1.0450279\n"
     ]
    }
   ],
   "source": [
    "for i in range(400):\n",
    "    total_cost, _ = sess.run([cost,optimize], feed_dict={x:X , y:Y_ohe})\n",
    "    print(total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = tf.argmax(pred, 1)\n",
    "correct_labels = tf.argmax(y_test, 1)\n",
    "correct_predictions = tf.equal(predictions, correct_labels)\n",
    "predictions,correct_predictions  = sess.run([predictions, correct_predictions], feed_dict={x:x_test,y:y_test})\n",
    "correct_predictions.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = correct_predictions.sum()/len(y_test)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
